# LLM Inference Stack

This project demonstrates deploying a lightweight ONNX-optimized LLM for fast inference using FastAPI and ONNX Runtime.

## Features

- ğŸ”¥ Fast inference using ONNX Runtime
- ğŸ§  Tokenization via Hugging Face Transformers
- ğŸ³ Dockerized for portability
- âš™ï¸ Designed to scale via Kubernetes

## Usage

1. Install dependencies:

```bash
pip install -r requirements.txt
