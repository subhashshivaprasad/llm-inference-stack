# LLM Inference Stack

This project demonstrates deploying a lightweight ONNX-optimized LLM for fast inference using FastAPI and ONNX Runtime.

## Features

- QUick inference using ONNX Runtime
- Tokenization via Hugging Face Transformers
- Dockerized for portability
-  Designed to scale via Kubernetes

## Usage

1. Install dependencies:

```bash
pip install -r requirements.txt
